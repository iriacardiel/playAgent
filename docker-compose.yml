# docker-compose.yml

services:
    
  ollama:
    # Build from your custom Dockerfile (which COPYs ./.ollama into /root/.ollama)
    build:
      context: ./ollama-custom        # <- folder that has Dockerfile and .ollama/
      dockerfile: Dockerfile
    image: agent-ollama-i:2025-09-08  # tag the result so it's easy to reference/save
    container_name: agent-ollama-c
    pull_policy: build
    restart: unless-stopped
    ports: ["11434:11434"]
    environment:
      OLLAMA_KEEP_ALIVE: "24h"
    gpus: all # UNCOMMENT if you have nvidia-docker set up
    #volumes: # No volumes: image is fully self-contained


  backend:
      build:
        context: ./backend
        dockerfile: Dockerfile
      image: agent-backend-i:2025-09-08  # tag the result so it's easy to reference/save
      container_name: agent-backend-c
      pull_policy: build
      restart: unless-stopped
      ports: ["2024:2024"] 
      environment:
        OLLAMA_HOST: http://ollama:11434
        PORT: "2024"
      #volumes: # No volumes: image is fully self-contained

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: agent-frontend-i:2025-09-08
    container_name: agent-frontend-c
    pull_policy: build
    restart: unless-stopped
    ports: ["3003:3003"]
    environment:
      NODE_ENV: production
      NEXT_TELEMETRY_DISABLED: "1"
      PORT: "3003"
      LANGGRAPH_API_URL: http://backend:2024
    #volumes: # No volumes: image is fully self-contained
