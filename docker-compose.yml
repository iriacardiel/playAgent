# docker-compose.yml

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports: ["11434:11434"]
    environment:
      OLLAMA_KEEP_ALIVE: "24h"
    volumes:
      - /home/ia-nsdas/.ollama:/root/.ollama
    gpus: all # install NVIDIA Container Toolkit
    
  backend:
      build:
        context: ./backend
        dockerfile: Dockerfile
      container_name: tars-backend
      restart: unless-stopped
      working_dir: /app
      environment:
        OLLAMA_HOST: http://ollama:11434
        PORT: "2024"
        PYTHONPATH: /app/src
      command: >
        /opt/venv/bin/langgraph dev
        --config ./langgraph.json
        --host 0.0.0.0
        --port ${PORT:-2024}
        --allow-blocking
        --no-browser
      ports:
        - "2024:2024"
      volumes:
        - ./backend:/app # TODO: revisar si hace falta

  frontend:
    build:
      context: ./frontend
      target: base
    container_name: tars-frontend
    restart: unless-stopped
    working_dir: /app
    environment:
      NODE_ENV: development
      NEXT_TELEMETRY_DISABLED: "1"
    command: sh -c "pnpm install && pnpm dev -p 3003 -H 0.0.0.0" # Dev: mirrors `cd frontend && pnpm dev` pending for Prod: standalone Next.js server
    ports:
      - "3003:3003"
    volumes:
      - ./frontend:/app # TODO: revisar si hace falta
      - web_node_modules:/app/node_modules # TODO: revisar si hace falta
      - web_next_cache:/app/.next # TODO: revisar si hace falta

#   # web: # TODO: revisar si hace falta
#   #   build:
#   #     context: ./frontend
#   #     target: runner
#   #   ports:
#   #     - "3003:3003"
#   #   environment:
#   #     NODE_ENV: production
#   #     NEXT_TELEMETRY_DISABLED: "1"
#   #   restart: unless-stopped

volumes:
  chroma_data:  # backend volume # TODO: revisar si hace falta
  web_node_modules: # frontend volume # TODO: revisar si hace falta
  web_next_cache: # frontend volume # TODO: revisar si hace falta


