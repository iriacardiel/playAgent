# docker-compose.yml

services:
    
  ollama:
    # Build from your custom Dockerfile (which COPYs ./.ollama into /root/.ollama)
    build:
      context: ./ollama-custom        # <- folder that has Dockerfile and .ollama/
      dockerfile: Dockerfile
    image: agent-ollama-i:latest  # tag the result so it's easy to reference/save
    container_name: agent-ollama-c
    restart: unless-stopped
    ports: ["${OLLAMA_PORT:-11434}:${OLLAMA_PORT:-11434}"]
    environment:
      OLLAMA_KEEP_ALIVE: "24h"
    gpus: all # UNCOMMENT if you have nvidia-docker set up
    #volumes: # No volumes: image is fully self-contained


  backend:
      build:
        context: ./backend
        dockerfile: Dockerfile
      image: agent-backend-i:latest  # tag the result so it's easy to reference/save
      container_name: agent-backend-c
      restart: unless-stopped
      ports: ["${BACKEND_PORT:-2024}:${BACKEND_PORT:-2024}"]
      environment:
        OLLAMA_HOST: http://ollama:${OLLAMA_PORT:-11434}
        PORT: "${BACKEND_PORT:-2024}"
      #volumes: # No volumes: image is fully self-contained

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        LANGGRAPH_API_URL: http://backend:${BACKEND_PORT:-2024}  # or whatever you want at build time
    image: agent-frontend-i:latest
    container_name: agent-frontend-c
    restart: unless-stopped
    ports: ["${FRONTEND_PORT:-3000}:${FRONTEND_PORT:-3000}"]
    environment:
      NODE_ENV: production
      NEXT_TELEMETRY_DISABLED: "1"
      PORT: "${FRONTEND_PORT:-3000}"
      LANGGRAPH_API_URL: http://backend:${BACKEND_PORT:-2024}
    #volumes: # No volumes: image is fully self-contained
