from langgraph import Node
from agent.prompts import get_judge_prompt

class Judge(Node):
    def __init__(self, judge_type: str, input_text: str):
        # judge_type could be 'user_input' or 'llm_output' or any custom type
        self.judge_type = judge_type
        self.input_text = input_text
        
    def run(self):
        # TODO ver si la distinción se hace aquí o a nivel del grafo
        if self.judge_type == 'user_input':
            is_safe = judge_content(self.input_text)
            return judge_node(is_safe)

        elif self.judge_type == 'llm_output':
            is_safe = judge_content(self.input_text)
            return judge_node(is_safe)
        
    def judge_content(input_text: str) -> bool:
        """Returns True if content is safe, False if it violates rules."""
        prompt = [SystemMessage(content=get_judge_prompt()), input_text]
        response = llm.invoke(prompt)
        return response.strip().upper() == "SAFE"


    def judge_node(input_text):
        if judge_content(input_text):
            return input_text
        else:
            return "⚠️ Content blocked due to safety concerns."
